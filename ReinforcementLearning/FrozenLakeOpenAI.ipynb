{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shaunak Prabhu\n",
    "\n",
    "NUID: 001056494 \n",
    "\n",
    "# Frozen Lake V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This Notebook takes the frozen lake problem in open-ai gym and uses value iteration to solve it. The frozen lake problem has a reward system of 1 for successfully reaching the goal and 0 for failing. This is a sparse reward system hence requiring much more episodes to have a higher success rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size,action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000  #total episodes during training\n",
    "max_steps_per_episode = 99 #max steps it can take during an episode\n",
    "\n",
    "learning_rate = 0.5 #alpha\n",
    "discount_rate = 0.8 #gamma\n",
    "\n",
    "exploration_rate = 1 #epsilon\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode\n",
      "1000 :  0.04500000000000003\n",
      "2000 :  0.05300000000000004\n",
      "3000 :  0.08800000000000006\n",
      "4000 :  0.07200000000000005\n",
      "5000 :  0.028000000000000018\n",
      "6000 :  0.04200000000000003\n",
      "7000 :  0.023000000000000013\n",
      "8000 :  0.02100000000000001\n",
      "9000 :  0.017000000000000008\n",
      "10000 :  0.014000000000000005\n",
      "update q_table\n",
      "[[1.18170488 1.18213982 1.18221366 1.18243306]\n",
      " [0.15364509 0.61099231 0.60347638 1.18243306]\n",
      " [1.16224724 1.14977634 1.11930913 1.18243306]\n",
      " [0.77388612 0.87322537 1.09802898 1.18243306]\n",
      " [1.18185716 0.35458179 0.24515541 0.29081854]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.67819819 0.0739383  0.06966948 0.06296292]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51252996 0.54663337 0.76972088 1.18140747]\n",
      " [0.66961382 1.17891207 0.34375692 0.55207884]\n",
      " [1.19418266 0.27243657 0.41899903 0.6127008 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.61861642 0.68207196 1.19038147 0.7689367 ]\n",
      " [1.03061077 1.23126591 1.1013242  1.0679828 ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "\n",
    "for episode in range (num_episodes):\n",
    "    state = env.reset()  #resets environment to starting state\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range (max_steps_per_episode):\n",
    "        \n",
    "        #exploration-exploitation tradeoff\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold>exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])  #chooses action with highest q value\n",
    "        else:\n",
    "            action = env.action_space.sample() #makes random action\n",
    "        new_state, reward, done, info = env.step(action)  #step calls the action and passes it\n",
    "        \n",
    "       # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * (np.max(q_table[new_state, :])-q_table[state,action]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward #updating rewards \n",
    "        \n",
    "        if done == True: #checks if episode is over, if so it finsihes run and moves to next episode\n",
    "            break\n",
    "            \n",
    "        #exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        #exploration rate is decayed propartional to its curretn value\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "#calculate and print average rewards per episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per episode\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count+=1000\n",
    "    \n",
    "#updated q table\n",
    "print(\"update q_table\")\n",
    "print(q_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size,action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000  #total episodes during training\n",
    "max_steps_per_episode = 100 #max steps it can take during an episode\n",
    "\n",
    "learning_rate = 0.05 #alpha\n",
    "discount_rate = 1 #gamma\n",
    "\n",
    "exploration_rate = 1 #epsilon\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "steps_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode\n",
      "1000 :  0.03400000000000002\n",
      "2000 :  0.1380000000000001\n",
      "3000 :  0.3900000000000003\n",
      "4000 :  0.5390000000000004\n",
      "5000 :  0.6430000000000005\n",
      "6000 :  0.6550000000000005\n",
      "7000 :  0.6860000000000005\n",
      "8000 :  0.6840000000000005\n",
      "9000 :  0.6800000000000005\n",
      "10000 :  0.7040000000000005\n",
      "Average number of steps per episode\n",
      "1000 :  8.049999999999967\n",
      "2000 :  15.567999999999957\n",
      "3000 :  27.6569999999999\n",
      "4000 :  34.52599999999994\n",
      "5000 :  37.530999999999956\n",
      "6000 :  38.97999999999999\n",
      "7000 :  38.889999999999915\n",
      "8000 :  38.358\n",
      "9000 :  36.94099999999992\n",
      "10000 :  38.585999999999956\n",
      "update q_table\n",
      "[[0.79753074 0.74247492 0.73634666 0.7452313 ]\n",
      " [0.36951587 0.2798961  0.2416851  0.70209845]\n",
      " [0.56872371 0.38468768 0.29294096 0.39460992]\n",
      " [0.03493103 0.06083168 0.0173468  0.38522385]\n",
      " [0.79691127 0.56529989 0.42818308 0.52101944]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28289131 0.16863473 0.43645792 0.09196543]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.58901124 0.56763906 0.48929179 0.79265373]\n",
      " [0.58856338 0.78018055 0.55879481 0.6168742 ]\n",
      " [0.70715806 0.50556354 0.46711473 0.39931374]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51337825 0.50438098 0.87113647 0.62769985]\n",
      " [0.82346959 0.92570603 0.85852992 0.83020986]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range (num_episodes):\n",
    "    state = env.reset()  #resets environment to starting state\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    num = 0\n",
    "    for step in range (max_steps_per_episode):\n",
    "        \n",
    "        #exploration-exploitation tradeoff\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold>exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])  #chooses action with highest q value\n",
    "        else:\n",
    "            action = env.action_space.sample() #makes random action\n",
    "        new_state, reward, done, info = env.step(action)  #step calls the action and passes it\n",
    "        \n",
    "       # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action]+learning_rate*(reward + discount_rate * (np.max(q_table[new_state, :])-q_table[state, action]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward #updating rewards \n",
    "        \n",
    "        if done == True: #checks if episode is over, if so it finsihes run and moves to next episode\n",
    "            num_current=step\n",
    "            break\n",
    "            \n",
    "        #exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        #exploration rate is decayed propartional to its curretn value\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    steps_all.append(num_current)\n",
    "#num = \n",
    "#calculate and print average rewards per episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "steps_per_thousand = np.split(np.array(steps_all), num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"Average reward per episode\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count+=1000\n",
    "\n",
    "flag = 1000\n",
    "print(\"Average number of steps per episode\")\n",
    "for s in steps_per_thousand:\n",
    "    print(flag, \": \", str(sum(s/1000)))\n",
    "    flag+=1000\n",
    "    \n",
    "#updated q table\n",
    "print(\"update q_table\")\n",
    "print(q_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The improved model has a much better performance as can be seen by the average rewards. Here the rewards are 1 for successfully reaching the goal and a 0 for failing to. Hence the average rewards is a an accurate indication of the accuracy of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1) How well did your RL Q-learning do on your problem ?\n",
    "\n",
    "- It performed far better than the baseline model in terms of accuracy and by increasing the no. of episodes it took to solve the problem. \n",
    "\n",
    "2) What are the states, actions, and size of the q-table?\n",
    "\n",
    "- The states are the squares in the frozen lake. These are the Frozen squares, Holes, and the Goal. The actions are the directions which the agent can move in: up, down, left, and right. Therefore the total number size of the q table is 4x16 = 64 \n",
    "\n",
    "3) What are the rewards ? why did you choose them?\n",
    "\n",
    "- The rewards are predefined by the open-ai gym which is 1 for every tim eit succesfully reaches the gial and 0 for every time it fails to. \n",
    "\n",
    "4) How did you choose alpha and gamma ?\n",
    "\n",
    "- alpha is the learing rate and was chosen as a very low value to make sure the agent explores the state space fully before using the values in the q-table to choose a route. Gamma determines how mnay states in the future must the agent look at. In this case we choose a high value for gamma as the problem is discrete therefore we can take as many valiues in the future. If the value for gamma were lower it would penalise future states more.\n",
    "\n",
    "5) Try a policy other than maxQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode\n",
      "1000 :  0.03300000000000002\n",
      "2000 :  0.10300000000000008\n",
      "3000 :  0.22700000000000017\n",
      "4000 :  0.23500000000000018\n",
      "5000 :  0.2750000000000002\n",
      "6000 :  0.5160000000000003\n",
      "7000 :  0.17700000000000013\n",
      "8000 :  0.2980000000000002\n",
      "9000 :  0.20800000000000016\n",
      "10000 :  0.5370000000000004\n",
      "update q_table\n",
      "[[3.71837780e-03 2.87142238e-03 2.98783452e-03 2.58469157e-03]\n",
      " [9.19310925e-04 1.74097691e-03 1.78217251e-03 2.78805145e-03]\n",
      " [4.39325854e-03 3.06177160e-03 3.40401519e-03 1.48708715e-03]\n",
      " [7.35228016e-05 2.21512890e-04 3.33922771e-05 1.32071201e-03]\n",
      " [6.61037931e-03 4.80547803e-03 3.97780162e-03 2.68420107e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.29857195e-02 2.20466099e-02 7.82866239e-03 3.85081339e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.99296867e-03 9.72220281e-03 9.72705667e-03 1.78439450e-02]\n",
      " [2.67514074e-02 4.65746342e-02 3.11576484e-02 2.75472859e-02]\n",
      " [5.21029877e-02 1.79651461e-01 6.40432109e-02 1.34389800e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.04862294e-02 8.31952956e-02 1.42124875e-01 7.20130187e-02]\n",
      " [1.28219117e-01 4.49203683e-01 2.95058419e-01 2.57938094e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range (num_episodes):\n",
    "    state = env.reset()  #resets environment to starting state\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    num=0\n",
    "    for step in range (max_steps_per_episode):\n",
    "        \n",
    "        #exploration-exploitation tradeoff\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold>exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])  #chooses action with highest q value\n",
    "        else:\n",
    "            action = env.action_space.sample() #makes random action\n",
    "        new_state, reward, done, info = env.step(action)  #step calls the action and passes it\n",
    "        \n",
    "       # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action]+learning_rate*(reward + discount_rate * (np.mean(q_table[new_state, :])-q_table[state, action]))\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward #updating rewards \n",
    "        \n",
    "        if done == True: #checks if episode is over, if so it finsihes run and moves to next episode\n",
    "            break\n",
    "            \n",
    "        #exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        #exploration rate is decayed propartional to its curretn value\n",
    "        \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "#calculate and print average rewards per episode\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per episode\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count+=1000\n",
    "    \n",
    "#updated q table\n",
    "print(\"update q_table\")\n",
    "print(q_table)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that by using the mean value gives a lower average reward than the max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) How did you choose your starting decay rate and starting epsilon? Try atleast one additional value for epsilon and the decay rate. How did it change the baseline performance ? what is the value of epsilon when your reach the max steps per episode.\n",
    "\n",
    "- The value of epsilon originally taken is 1 as the agent needs to explore the state space as the q table is initially 0. The decay rate is small as the reward system is sparse therfore allowing the agent to explore most of the environment before getting a reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What is the average number of steps taken per episode\n",
    "\n",
    "- on average 38 steps are taken per episode in the improved model. \n",
    "\n",
    "8) Does Q-learning use value based or policy based iteration?\n",
    "\n",
    "- q-learning uses value iteration as the q table is being updated each time based on the best reward it receives. \n",
    "\n",
    "9) What is meant by expected lifetime value in Bellman equation?\n",
    "\n",
    "- the expected lifetime value is the value multiplied by our discount rate which determines how many state-action pairs in the future we look at to determine our updated q-table value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Shaunak Prabhu \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
